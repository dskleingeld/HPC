%
% verslag.tex   18.2.2019
% Voorbeeld LaTeX-file voor verslagen bij Kunstmatige Intelligentie
% http://www.liacs.leidenuniv.nl/~kosterswa/AI/verslag.tex
%
% Gebruik:
%   pdflatex verslag.tex
%

\documentclass[10pt]{article}

\parindent=0pt

\usepackage{fullpage}

\frenchspacing
\usepackage{array} 
\usepackage{microtype}
\usepackage{scrextend}
\usepackage[english,english]{babel}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{amsmath}
%\usepackage{listings}
\usepackage{subcaption}

% Er zijn talloze parameters ...
%\lstset{language=C++, showstringspaces=false, basicstyle=\small,
%  numbers=left, numberstyle=\tiny, numberfirstline=false, breaklines=true,
%  stepnumber=1, tabsize=8,
%  commentstyle=\ttfamily, identifierstyle=\ttfamily,
%  stringstyle=\itshape}

%\usepackage[setpagesize=false,colorlinks=true,linkcolor=red,urlcolor=blue,pdftitle={Het grote probleem},pdfauthor={David Kleingeld}]{hyperref}

\author{David Kleingeld, s1432982}
\title{}

\begin{document}

\selectlanguage{english}

\maketitle

\section{Introduction}
Many problems in science depend on solving linear systems. To speed up computations they depend on LU factorization. This can be viewed as a matrix form of Gaussian elimination. LU factorization was introduced by polish mathematician Tadeusz Banachiewicz \cite{lu}.
Often these matrices are extremely large while containing few non zero entries, so called sparse matrices. The LU factorization can be sped up by skipping these entries. There are a number of schemes to store only non zero entries for sparse matrices. The simplest, Dictionary of keys, stores the value row and column for each non zero entry. Here we have implemented a Sparse LU Factorization kernel using compressed row storage. With compressed row storage only the non zero entries in each rows are stored. The order of the entries is preserved and for each entry the column is kept with it \cite{compressedRowStorage}. We discuss the implementation, how we validated it, how we benchmark, the results followed by a discussion of these where we also point out possible improvements. 

\section{Implementation}

During Lu factoring we traverse the columns of the matrix, for each column we designate a pivot, the element on the diagonal in that column. We add the row with this pivot to the rows below it scaled such that the elements below the pivot become zero. This will be problematic when the pivot value is small while the values below it are large. Then the rows will be scaled by a very large factor. Future pivot values will now have extremer values reducing numerical precision. 
We can mitigate most of this using partial pivoting. Here for each pivot we check if there is a row with a larger value in the column below the current pivot. If there is we exchange rows so the larger value is now the current pivot. This way we mostly do not scale by large factors. We keep a list of applied row interchanges, using it we can still solve the system.

Another difficulty is the actual adding of two rows. This being key to the algorithm it happens often. When we add two sparse rows more non zero elements can appear in the resulting row. However as we keep changing elements below pivots to zero non zero values will also disappear. We store the compressed rows after one another, thus expanding is not an option. Instead, when a row needs to expand we move after the last row. This creates gaps in between compressed rows reducing data locality which reduces performance and increasing the required memory to store the rows. The solution is to every once in a while move all rows to a new memory space. Here we put them against eachother again.

\section{Validation}
\label{sec:val}
We validate the correct operation of the LU factorization by using it to solve known linear systems. For this we use a number of matrices from https://math.nist.gov/MatrixMarket/. By taking the dot product of known solution vectors with these matrices we create a system of equation of the matrix and the outcome of the dot product. Using the LU factorization we solve this system. If the solution matches the known solution vector our LU factorization must be correct. 

For each system we try we store the relative errors: $\frac{||\widetilde{x}-x||}{||x||}$. Here $\widetilde{x}$ is the solution computed by our implementation and $x$ the actual solution known from the solution vector. $||x||$ denotes the Euclidean Norm: $\sqrt{\Sigma x_i^2}$.

We used the following matrices from Matrix-Market:
\begin{multicols}{2}
\begin{enumerate}
    \item HB/mcfe
    \item Schenk\_IBMNA/c-21
    \item Oberwolfach/flowmeter5
    \item Averous/epb1
    \item Grund/meg4
    \item Lucifora/cell1
    \item Gaertner/nopoly
    \item Bai/mhd4800b
    \item FIDAP/ex10
    \item Okunbor/aft01
\end{enumerate}
\end{multicols}

We gave the solution vectors five different values:

\begin{description}
    \item [ones] all ones
    \item [point ones] all value $0.1$
    \item [alternating ones] all alternating $+1$ and $-1$
    \item [alternating fives] all alternating $+5$ and $-5$
    \item [alternating hundreds] all alternating $+100$ and $-100$
\end{description}

\section{Benchmark}
Finally for each of the matrices we benchmark the time to factorize the and to then solve each of the solution vectors with the factored system. The benchmarks where carried out on the Linux systems on the Core i7 machines in room 302/304 at the LIACS.

\section{Results}
In \autoref{tab:errors} we see the relative error per solution vector per matrix as defined in \autoref{sec:val}. Then in \autoref{tab:factoring} the result of benchmarking the factoring of the different matrices. The measured run time is in seconds. Finally in \autoref{tab:solving} we have the 

\begin{table}
    \begin{tabular}{ l c c c c c }
    \firsthline
    \multicolumn{6}{c}{Relative Error (seconds)} \\
    \cline{2-6}
    Matrix & ones & point ones & alternating ones & alternating fives & alternating hundreds \\
    \hline
    HB/mcfe & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    Schenk\_IBMNA/c-21 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    Oberwolfach/flowmeter5 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    Averous/epb1 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    Grund/meg4 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    Lucifora/cell1 & 0.002045 & 0.001476 & 0.001259 & 0.001211 & 0.001189 \\
    Gaertner/nopoly & infinite & 1.908226 & 0.021489 & 0.012033 & 0.007542 \\
    Bai/mhd4800b & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    FIDAP/ex10 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    Okunbor/aft01 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
    \lasthline
    \end{tabular}
    \caption{Relative error for all the solution vectors for all matrices.}
    \label{tab:errors}
\end{table}

\begin{table}
    \begin{tabular}{ l c c c }
    \firsthline
    Matrix & Factoring time (seconds) & Matrix Size & Number of non zeros \\
    \hline
    HB/mcfe & 0.079421 & 765x765 & 24382\\
    Schenk\_IBMNA/c-21 & 21.411541 & 3509x3509 & 17833\\
    Oberwolfach/flowmeter5 & 5.505239 & 9669x9669 & 67391\\
    Averous/epb1 & 18.281949 & 14734x14734 & 95053\\
    Grund/meg4 & 2.678350 & 5860x5860 & 26324\\
    Lucifora/cell1 & 1.785907 & 7055x7055 & 34855\\
    Gaertner/nopoly & 136.847555 & 10774x10774 & 40808\\
    Bai/mhd4800b & 0.161034 & 4800x4800 & 16160\\
    FIDAP/ex10 & 0.429020 & 2410x2410 & 28625\\
    Okunbor/aft01 & 7.076001 & 8205x8205 & 66886\\
    \lasthline
    \end{tabular}
    \caption{Time it takes to LU-Factor different matrices in seconds. Compared to their size and number of non zeros.}
    \label{tab:factoring}
\end{table}

\begin{table}
    \begin{tabular}{ l c c c c c }
    \firsthline
    \multicolumn{6}{c}{Solving Runtime} \\
    \cline{2-6}
    Matrix & ones & point ones & alternating ones & alternating fives & alternating hundreds \\
    \hline
    HB/mcfe & 0.000239 & 0.000180 & 0.000178 & 0.000178 & 0.000179 \\
    Schenk\_IBMNA/c-21 & 0.011501 & 0.011447 & 0.011552 & 0.011226 & 0.011222 \\
    Oberwolfach/flowmeter5 & 0.003515s & 0.003044 & 0.002929 & 0.002888 & 0.002866 \\
    Averous/epb1 & 0.008963 & 0.009685 & 0.008836 & 0.009996 & 0.008885 \\
    Grund/meg4 & 0.002010 & 0.001674 & 0.001583 & 0.001562 & 0.001550 \\
    Lucifora/cell1 & 0.000158 & 0.000147 & 0.000038 & 0.000040 & 0.000026 \\
    Gaertner/nopoly & 0.034487 & 0.033298 & 0.033214 & 0.033124 & 0.032524 \\
    Bai/mhd4800b & 0.000149 & 0.000066 & 0.000066 & 0.000065 & 0.000065 \\
    FIDAP/ex10 & 0.000564 & 0.000398 & 0.000386 & 0.000385 & 0.000383 \\
    Okunbor/aft01 & 0.005008 & 0.004737 & 0.005241 & 0.004814 & 0.004735 \\
    \lasthline
    \end{tabular}
    \caption{Runtime solving the different solution vectors for all matrices in seconds.}
    \label{tab:solving}
\end{table}

\section{Discussion}
We see our implementation fails to correctly factor or solve Lucifora/cell1 or Gaertner/nopoly. Nopoly with the all ones solution vector even has an extreme error. The nopoly matrix has the highest run time at $137$ seconds. This is surprising as Averous/epb1 has an even larger size and more non zeros. We expect the non zeros are spread over different columns causing a lot of non zeros to appear during the factorization. Gaertner/nopoly also has the largest run time for solving the different systems. This is expected. The smallest matrix has the lowest factoring time while matrices with a lower number of non zeros have a dramatically higher run time. This is again caused by many more non zeros appearing during the factoring.

Finally we see a number of possible optimization to the current implementation:
\begin{enumerate}
\item As the density of the matrices increases during factoring the relative overhead of working in compressed row format grows. Switching to a normal dense matrix after a critical point will provide a speedup in factoring and solving. 

\item Currently rows need to be moved as soon as one element needs to be added. We can add some extra unused space between rows to allow them to grow without having to move them. 

\item When adding two rows together they are added into a dense row. However when broadcasting the values back into a compressed row we need to iterate over the complete dense row to find the non zero values. Keeping a list of non zeros in the dense row would prevent this and speed up the factoring.

\item Currently the values and column indexes are stored in large continues storage. However both are often used simultaneously. We might get a speedup if we store value and corresponding column as a pair in a single continues storage. 
\end{enumerate}

\clearpage
\bibliography{main.bib}
\bibliographystyle{IEEEtran}

\end{document}